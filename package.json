{
  "name": "coqpilot",
  "displayName": "Ð¡oqPilot",
  "description": "An AI-based completion extension for Coq interactive prover.",
  "icon": "etc/img/logo.ico",
  "repository": {
    "type": "git",
    "url": "https://github.com/JetBrains-Research/coqpilot"
  },
  "publisher": "JetBrains-Research",
  "version": "2.5.0",
  "engines": {
    "vscode": "^1.95.0"
  },
  "categories": [
    "Other"
  ],
  "activationEvents": [
    "onLanguage:coq"
  ],
  "main": "./out/mainNode.js",
  "contributes": {
    "languages": [
      {
        "id": "coq",
        "aliases": [
          "Coq",
          "coq",
          "Gallina",
          "gallina"
        ],
        "extensions": [
          ".v"
        ]
      }
    ],
    "commands": [
      {
        "command": "coqpilot.perform_completion_under_cursor",
        "title": "CoqPilot: Try to generate proof for the goal under the cursor"
      },
      {
        "command": "coqpilot.perform_completion_for_all_admits",
        "title": "CoqPilot: Try to prove all holes (admitted goals) in the current file"
      },
      {
        "command": "coqpilot.perform_completion_in_selection",
        "title": "CoqPilot: Try to prove holes (admitted goals) in the selection"
      }
    ],
    "keybindings": [
      {
        "command": "coqpilot.perform_completion_in_selection",
        "key": "ctrl+shift+[BracketRight]",
        "mac": "shift+cmd+[BracketRight]"
      }
    ],
    "menus": {
      "editor/context": [
        {
          "command": "coqpilot.perform_completion_in_selection",
          "when": "editorTextFocus && editorHasSelection && resourceLangId == coq",
          "group": "queries"
        }
      ]
    },
    "configuration": [
      {
        "type": "object",
        "title": "CoqPilot",
        "properties": {
          "coqpilot.predefinedProofsModelsParameters": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "modelId": {
                  "type": "string",
                  "markdownDescription": "Unique identifier of this model to distinguish it from others. Could be any string.",
                  "default": "predefined-auto"
                },
                "tactics": {
                  "type": "array",
                  "items": {
                    "type": "string"
                  },
                  "description": "List of tactics to try to complete a hole with. Commands in the list must be valid Coq commands available in your environment.",
                  "default": [
                    "auto."
                  ]
                }
              }
            },
            "default": [
              {
                "modelId": "predefined-auto",
                "tactics": [
                  "auto."
                ]
              }
            ],
            "markdownDescription": "List of configurations with sets of predefined proofs. CoqPilot will check these proofs when searching for completion.",
            "order": 0
          },
          "coqpilot.openAiModelsParameters": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "modelId": {
                  "type": "string",
                  "markdownDescription": "Unique identifier of this model to distinguish it from others. Could be any string.",
                  "default": "gpt-4o-mini"
                },
                "modelName": {
                  "type": "string",
                  "markdownDescription": "Model to use from the OpenAI platform. List of models known to CoqPilot: \n * gpt-4o \n * gpt-4o-2024-08-06 \n * gpt-4o-2024-05-13 \n * gpt-4o-mini \n * gpt-4o-mini-2024-07-18 \n * gpt-4-turbo \n * gpt-4-turbo-2024-04-09 \n * gpt-4-turbo-preview \n * gpt-4-0125-preview \n * gpt-4-1106-preview \n * gpt-4 \n * gpt-4-0613 \n * gpt-4-0314 \n * gpt-3.5-turbo-0125 \n * gpt-3.5-turbo \n * gpt-3.5-turbo-1106 \n * gpt-3.5-turbo-instruct",
                  "default": "gpt-4o-mini"
                },
                "temperature": {
                  "type": "number",
                  "description": "Temperature of the OpenAI model. Should be in range [0, 2], otherwise an error will be produced.",
                  "default": 1
                },
                "apiKey": {
                  "type": "string",
                  "description": "Api key to communicate with the OpenAi api. You can get one [here](https://platform.openai.com/account/api-keys).",
                  "default": "None"
                },
                "choices": {
                  "type": "number",
                  "description": "Number of attempts to generate proof for one hole with this model. All attempts are made as a single request, so this parameter should not have a significant impact on performance. However, more choices mean more tokens spent on generation.",
                  "default": 15
                },
                "systemPrompt": {
                  "type": "string",
                  "description": "Prompt for the OpenAI model to begin a chat with. It is sent as a system message, which means it has more impact than other messages.",
                  "default": "Generate proof of the theorem from user input in Coq. You should only generate proofs in Coq. Never add special comments to the proof. Your answer should be a valid Coq proof. It should start with 'Proof.' and end with 'Qed.'."
                },
                "maxTokensToGenerate": {
                  "type": "number",
                  "description": "Number of tokens that the model is allowed to generate as a response message (i.e. message with proof). For known models, CoqPilot provides a recommended default value, but it can be customized for more advanced proof generation. The default value is the maximum allowed value for the model if it takes no more than half of `tokensLimit`, otherwise the minimum of half of `tokensLimit` and 4096.",
                  "default": 2048
                },
                "tokensLimit": {
                  "type": "number",
                  "description": "Total length of input and generated tokens, it is determined by the model. For known models, CoqPilot provides a recommended default value (the maximum model context length), but it can be customized for more advanced proof generation.",
                  "default": 4096
                },
                "maxContextTheoremsNumber": {
                  "type": "number",
                  "description": "Maximum number of context theorems to include in the prompt sent to the OpenAI model as examples for proof generation. Lower values reduce token usage but may decrease the likelihood of generating correct proofs.",
                  "default": 100
                },
                "multiroundProfile": {
                  "type": "object",
                  "properties": {
                    "maxRoundsNumber": {
                      "type": "number",
                      "description": "Maximum number of rounds to generate and further fix the proof. Default value is 1, which means each proof will be only generated, but not fixed.",
                      "default": 1
                    },
                    "proofFixChoices": {
                      "type": "number",
                      "description": "Number of attempts to generate a proof fix for each proof in one round. Warning: increasing `proofFixChoices` can lead to exponential growth in generation requests if `maxRoundsNumber` is relatively large.",
                      "default": 1
                    },
                    "proofFixPrompt": {
                      "type": "string",
                      "description": "Prompt for the proof-fix request that will be sent as a user chat message in response to an incorrect proof. It may include the `${diagnostic}` substring, which will be replaced by the actual compiler diagnostic.",
                      "default": "Unfortunately, the last proof is not correct. Here is the compiler's feedback: `${diagnostic}`. Please, fix the proof."
                    },
                    "maxPreviousProofVersionsNumber": {
                      "type": "number",
                      "description": "Maximum number of previous proof versions to include in the proof-fix chat, each presented as a dialogue: the user's diagnostic followed by the assistant's corresponding proof attempt. The most recent proof version being fixed is always included and is not affected by this parameter.",
                      "default": 100
                    }
                  },
                  "default": {
                    "maxRoundsNumber": 1,
                    "proofFixChoices": 1,
                    "proofFixPrompt": "Unfortunately, the last proof is not correct. Here is the compiler's feedback: `${diagnostic}`. Please, fix the proof.",
                    "maxPreviousProofVersionsNumber": 100
                  }
                }
              }
            },
            "default": [
              {
                "modelId": "gpt-4o-mini",
                "modelName": "gpt-4o-mini",
                "temperature": 1,
                "apiKey": "None",
                "choices": 15,
                "systemPrompt": "Generate proof of the theorem from user input in Coq. You should only generate proofs in Coq. Never add special comments to the proof. Your answer should be a valid Coq proof. It should start with 'Proof.' and end with 'Qed.'.",
                "maxTokensToGenerate": 2048,
                "tokensLimit": 4096,
                "maxContextTheoremsNumber": 100,
                "multiroundProfile": {
                  "maxRoundsNumber": 1,
                  "proofFixChoices": 1,
                  "proofFixPrompt": "Unfortunately, the last proof is not correct. Here is the compiler's feedback: '${diagnostic}'. Please, fix the proof.",
                  "maxPreviousProofVersionsNumber": 100
                }
              }
            ],
            "markdownDescription": "List of configurations for OpenAI models. Each configuration will be fetched for completions independently in the order they are listed.",
            "order": 1
          },
          "coqpilot.grazieModelsParameters": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "modelId": {
                  "type": "string",
                  "markdownDescription": "Unique identifier of this model to distinguish it from others. Could be any string.",
                  "default": "openai-gpt-4o-via-grazie"
                },
                "modelName": {
                  "type": "string",
                  "markdownDescription": "Model to use from the Grazie platform: \n * openai-gpt-4 \n * openai-chat-gpt \n * grazie-chat-llama-v2-7b \n * grazie-chat-llama-v2-13b \n * grazie-chat-zephyr-7b \n * qwen-turbo \n * qwen-plus",
                  "default": "openai-gpt-4o"
                },
                "apiKey": {
                  "type": "string",
                  "description": "Api key to communicate with the Grazie api. Now available for JetBrains employees only.",
                  "default": "None"
                },
                "authType": {
                  "type": "string",
                  "description": "Use stgn if you are an internal JetBrains AI user and use prod otherwise.",
                  "default": "stgn"
                },
                "choices": {
                  "type": "number",
                  "description": "Number of attempts to generate proof for one hole with this model.",
                  "default": 15
                },
                "systemPrompt": {
                  "type": "string",
                  "description": "Prompt for the Grazie model to begin chat with. It is sent as a system message, which means it has more impact than other messages.",
                  "default": "Generate proof of the theorem from user input in Coq. You should only generate proofs in Coq. Never add special comments to the proof. Your answer should be a valid Coq proof. It should start with 'Proof.' and end with 'Qed.'."
                },
                "maxTokensToGenerate": {
                  "type": "number",
                  "description": "Number of tokens that the model is allowed to generate as a response message (i.e. message with proof).",
                  "default": 1024
                },
                "tokensLimit": {
                  "type": "number",
                  "description": "Total length of input and generated tokens, it is determined by the model. For OpenAI models, tokens limits could be found [here](https://platform.openai.com/docs/models/).",
                  "default": 4096
                },
                "maxContextTheoremsNumber": {
                  "type": "number",
                  "description": "Maximum number of context theorems to include in the prompt sent to the Grazie model as examples for proof generation. Lower values reduce token usage but may decrease the likelihood of generating correct proofs.",
                  "default": 100
                },
                "multiroundProfile": {
                  "type": "object",
                  "properties": {
                    "maxRoundsNumber": {
                      "type": "number",
                      "description": "Maximum number of rounds to generate and further fix the proof. Default value is 1, which means each proof will be only generated, but not fixed.",
                      "default": 1
                    },
                    "proofFixChoices": {
                      "type": "number",
                      "description": "Number of attempts to generate a proof fix for each proof in one round. Warning: increasing `proofFixChoices` can lead to exponential growth in generation requests if `maxRoundsNumber` is relatively large.",
                      "default": 1
                    },
                    "proofFixPrompt": {
                      "type": "string",
                      "description": "Prompt for the proof-fix request that will be sent as a user chat message in response to an incorrect proof. It may include the `${diagnostic}` substring, which will be replaced by the actual compiler diagnostic.",
                      "default": "Unfortunately, the last proof is not correct. Here is the compiler's feedback: `${diagnostic}`. Please, fix the proof."
                    },
                    "maxPreviousProofVersionsNumber": {
                      "type": "number",
                      "description": "Maximum number of previous proof versions to include in the proof-fix chat, each presented as a dialogue: the user's diagnostic followed by the assistant's corresponding proof attempt. The most recent proof version being fixed is always included and is not affected by this parameter.",
                      "default": 100
                    }
                  },
                  "default": {
                    "maxRoundsNumber": 1,
                    "proofFixChoices": 1,
                    "proofFixPrompt": "Unfortunately, the last proof is not correct. Here is the compiler's feedback: `${diagnostic}`. Please, fix the proof.",
                    "maxPreviousProofVersionsNumber": 100
                  }
                }
              }
            },
            "default": [],
            "markdownDescription": "Now available in beta for JetBrains employees only. List of configurations for Grazie models. Each configuration will be fetched for completions independently in the order they are listed.",
            "order": 2
          },
          "coqpilot.lmStudioModelsParameters": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "modelId": {
                  "type": "string",
                  "markdownDescription": "Unique identifier of this model to distinguish it from others. Could be any string.",
                  "default": "lm-studio"
                },
                "temperature": {
                  "type": "number",
                  "description": "Temperature of the LM Studio model. Should be in range [0, 2], otherwise an error will be produced.",
                  "default": 1
                },
                "port": {
                  "type": "number",
                  "description": "Port on which LM Studio is launched.",
                  "default": 1234
                },
                "choices": {
                  "type": "number",
                  "description": "Number of attempts to generate proof for one hole with this model.",
                  "default": 15
                },
                "systemPrompt": {
                  "type": "string",
                  "description": "Prompt for the LM Studio model to begin chat with. It is sent as a system message, which means it has more impact than other messages.",
                  "default": "Generate proof of the theorem from user input in Coq. You should only generate proofs in Coq. Never add special comments to the proof. Your answer should be a valid Coq proof. It should start with 'Proof.' and end with 'Qed.'."
                },
                "maxTokensToGenerate": {
                  "type": "number",
                  "description": "Number of tokens that the model is allowed to generate as a response message (i.e. message with proof).",
                  "default": 1024
                },
                "tokensLimit": {
                  "type": "number",
                  "description": "Total length of input and generated tokens, usually it is determined by the model.",
                  "default": 2048
                },
                "maxContextTheoremsNumber": {
                  "type": "number",
                  "description": "Maximum number of context theorems to include in the prompt sent to the LM Studio model as examples for proof generation. Lower values reduce token usage but may decrease the likelihood of generating correct proofs.",
                  "default": 100
                },
                "multiroundProfile": {
                  "type": "object",
                  "properties": {
                    "maxRoundsNumber": {
                      "type": "number",
                      "description": "Maximum number of rounds to generate and further fix the proof. Default value is 1, which means each proof will be only generated, but not fixed.",
                      "default": 1
                    },
                    "proofFixChoices": {
                      "type": "number",
                      "description": "Number of attempts to generate a proof fix for each proof in one round. Warning: increasing `proofFixChoices` can lead to exponential growth in generation requests if `maxRoundsNumber` is relatively large.",
                      "default": 1
                    },
                    "proofFixPrompt": {
                      "type": "string",
                      "description": "Prompt for the proof-fix request that will be sent as a user chat message in response to an incorrect proof. It may include the `${diagnostic}` substring, which will be replaced by the actual compiler diagnostic.",
                      "default": "Unfortunately, the last proof is not correct. Here is the compiler's feedback: `${diagnostic}`. Please, fix the proof."
                    },
                    "maxPreviousProofVersionsNumber": {
                      "type": "number",
                      "description": "Maximum number of previous proof versions to include in the proof-fix chat, each presented as a dialogue: the user's diagnostic followed by the assistant's corresponding proof attempt. The most recent proof version being fixed is always included and is not affected by this parameter.",
                      "default": 100
                    }
                  },
                  "default": {
                    "maxRoundsNumber": 1,
                    "proofFixChoices": 1,
                    "proofFixPrompt": "Unfortunately, the last proof is not correct. Here is the compiler's feedback: `${diagnostic}`. Please, fix the proof.",
                    "maxPreviousProofVersionsNumber": 100
                  }
                }
              }
            },
            "default": [],
            "markdownDescription": "List of configurations that fetch completions from a locally running LLM inside [LM Studio](https://lmstudio.ai).",
            "order": 3
          },
          "coqpilot.deepSeekModelsParameters": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "modelId": {
                  "type": "string",
                  "markdownDescription": "Unique identifier of this model to distinguish it from others. Could be any string.",
                  "default": "deep-seek-v3"
                },
                "modelName": {
                  "type": "string",
                  "markdownDescription": "Model to use from the DeepSeek public API. List of models known to CoqPilot: \n * deepseek-chat \n * deepseek-reasoner",
                  "default": "deepseek-chat"
                },
                "temperature": {
                  "type": "number",
                  "description": "Temperature of the DeepSeek model. Should be in range [0, 2], otherwise an error will be produced.",
                  "default": 1
                },
                "apiKey": {
                  "type": "string",
                  "description": "Api key to communicate with the DeepSeek api. You can get one [here](https://platform.deepseek.com/api_keys).",
                  "default": "None"
                },
                "choices": {
                  "type": "number",
                  "description": "Number of attempts to generate proof for one hole with this model. All attempts are made as a single request, so this parameter should not have a significant impact on performance. However, more choices mean more tokens spent on generation.",
                  "default": 15
                },
                "systemPrompt": {
                  "type": "string",
                  "description": "Prompt for the DeepSeek model to begin a chat with. It is sent as a system message, which means it has more impact than other messages.",
                  "default": "Generate proof of the theorem from user input in Coq. You should only generate proofs in Coq. Never add special comments to the proof. Your answer should be a valid Coq proof. It should start with 'Proof.' and end with 'Qed.'."
                },
                "maxTokensToGenerate": {
                  "type": "number",
                  "description": "Number of tokens that the model is allowed to generate as a response message (i.e. message with proof).",
                  "default": 2048
                },
                "tokensLimit": {
                  "type": "number",
                  "description": "Total length of input and generated tokens, it is determined by the model.",
                  "default": 4096
                },
                "maxContextTheoremsNumber": {
                  "type": "number",
                  "description": "Maximum number of context theorems to include in the prompt sent to the DeepSeek model as examples for proof generation. Lower values reduce token usage but may decrease the likelihood of generating correct proofs.",
                  "default": 100
                },
                "multiroundProfile": {
                  "type": "object",
                  "properties": {
                    "maxRoundsNumber": {
                      "type": "number",
                      "description": "Maximum number of rounds to generate and further fix the proof. Default value is 1, which means each proof will be only generated, but not fixed.",
                      "default": 1
                    },
                    "proofFixChoices": {
                      "type": "number",
                      "description": "Number of attempts to generate a proof fix for each proof in one round. Warning: increasing `proofFixChoices` can lead to exponential growth in generation requests if `maxRoundsNumber` is relatively large.",
                      "default": 1
                    },
                    "proofFixPrompt": {
                      "type": "string",
                      "description": "Prompt for the proof-fix request that will be sent as a user chat message in response to an incorrect proof. It may include the `${diagnostic}` substring, which will be replaced by the actual compiler diagnostic.",
                      "default": "Unfortunately, the last proof is not correct. Here is the compiler's feedback: `${diagnostic}`. Please, fix the proof."
                    },
                    "maxPreviousProofVersionsNumber": {
                      "type": "number",
                      "description": "Maximum number of previous proof versions to include in the proof-fix chat, each presented as a dialogue: the user's diagnostic followed by the assistant's corresponding proof attempt. The most recent proof version being fixed is always included and is not affected by this parameter.",
                      "default": 100
                    }
                  },
                  "default": {
                    "maxRoundsNumber": 1,
                    "proofFixChoices": 1,
                    "proofFixPrompt": "Unfortunately, the last proof is not correct. Here is the compiler's feedback: `${diagnostic}`. Please, fix the proof.",
                    "maxPreviousProofVersionsNumber": 100
                  }
                }
              }
            },
            "default": [],
            "markdownDescription": "List of configurations for DeepSeek models. Each configuration will be fetched for completions independently in the order they are listed.",
            "order": 4
          },
          "coqpilot.contextTheoremsRankerType": {
            "type": "string",
            "enum": [
              "distance",
              "random",
              "jaccardIndex"
            ],
            "markdownEnumDescriptions": [
              "Theorems are selected based on the distance to the current cursor position.",
              "Theorems are selected randomly.",
              "Theorems are selected based on the Jaccard index. This metric aims to pick the most similar theorems to the goal currenly being proved."
            ],
            "description": "Context of the LLM is limited. Usually not all theorems from the file may be used in the completion request. This parameter defines the way theorems are selected for the completion.",
            "default": "distance",
            "order": 5
          },
          "coqpilot.loggingVerbosity": {
            "type": "string",
            "enum": [
              "info",
              "debug"
            ],
            "markdownEnumDescriptions": [
              "Only important information is logged.",
              "All information is logged."
            ],
            "description": "The verbosity of the logs.",
            "default": "info",
            "order": 6
          },
          "coqpilot.coqLspServerPath": {
            "type": "string",
            "description": "Path to the Coq LSP server. If not specified, CoqPilot will try to find the server automatically at the default location: coq-lsp at PATH.",
            "default": "coq-lsp",
            "order": 7
          }
        }
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "lint": "eslint \"src/**/*.{ts,js}\"",
    "format": "prettier --write \"src/**/*.{ts,js}\" && eslint \"src/**/*.{ts,js}\" --fix",
    "test-executables-unsafe": "node ./out/test/runTest.js",
    "test-only": "npm run test-executables-unsafe -- -r=\"\\[SourceExecutable\\] *\" -i=true",
    "pretest": "npm run compile && npm run lint",
    "test": "npm run test-only",
    "clean": "rm -rf out",
    "rebuild": "npm run clean && npm run compile && npm run format",
    "rebuild-test-resources": "cd ./src/test/resources/coqProj && make clean && make",
    "preclean-test": "npm run clean && npm run rebuild-test-resources && npm run compile && npm run lint",
    "clean-test": "npm run test-only",
    "prebenchmark": "npm run preclean-test",
    "premulti-workspaces-benchmark": "npm run prebenchmark",
    "multi-workspaces-benchmark": "node ./out/benchmark/multiWorkspacesSetup.js",
    "presingle-workspace-benchmark": "npm run prebenchmark",
    "single-workspace-benchmark": "npm run test-executables-unsafe -- -g=\"[SourceExecutable] Single Workspace Benchmark\"",
    "benchmark": "npm run single-workspace-benchmark",
    "prebenchmark-test": "rm -rf benchmarksOutput",
    "benchmark-test": "npm run benchmark",
    "preteamcity-benchmark-setup": "npm run prebenchmark",
    "teamcity-benchmark-setup": "node ./out/benchmark/teamCitySetup.js",
    "teamcity-benchmark-agent": "npm run test-executables-unsafe -- -g=\"[SourceExecutable] Team City Benchmark Agent\"",
    "prelegacy-benchmark": "npm run prebenchmark",
    "legacy-benchmark": "npm run test-executables-unsafe -- -g=\"[SourceExecutable] Legacy Benchmark\""
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3.2.0",
    "@eslint/js": "^9.15.0",
    "@trivago/prettier-plugin-sort-imports": "^4.3.0",
    "@types/cli-progress": "^3.11.6",
    "@types/event-source-polyfill": "^1.0.5",
    "@types/expect.js": "^0.3.32",
    "@types/mocha": "^10.0.10",
    "@types/node": "22.10.0",
    "@types/node-ipc": "^9.2.3",
    "@types/tmp": "^0.2.6",
    "@types/vscode": "^1.95.0",
    "@types/yargs": "^17.0.33",
    "@typescript-eslint/eslint-plugin": "^8.0.0",
    "@typescript-eslint/parser": "^8.0.0",
    "@vscode/test-electron": "^2.4.1",
    "earl": "^1.3.0",
    "eslint": "^9.0.0",
    "eslint-config-prettier": "^9.1.0",
    "eslint-plugin-prettier": "^5.1.3",
    "glob": "11.0.0",
    "mocha": "^11.0.0",
    "prettier": "^3.4.1",
    "typescript": "^5.7.2"
  },
  "dependencies": {
    "@codemirror/autocomplete": "^6.18.3",
    "ajv": "^8.17.1",
    "async-mutex": "^0.5.0",
    "axios": "^1.7.8",
    "cli-progress": "^3.12.0",
    "dedent": "^1.5.3",
    "event-source-polyfill": "^1.0.31",
    "i": "^0.3.7",
    "mocha-param": "^2.0.1",
    "node-ipc": "^12.0.0",
    "npm": "^10.9.1",
    "openai": "^4.73.1",
    "path": "^0.12.7",
    "pino": "^9.5.0",
    "pino-pretty": "^13.0.0",
    "tiktoken": "^1.0.17",
    "tmp": "^0.2.3",
    "toml": "^3.0.0",
    "ts-results": "^3.3.0",
    "vscode-languageclient": "^9.0.1",
    "yargs": "^17.7.2"
  }
}
